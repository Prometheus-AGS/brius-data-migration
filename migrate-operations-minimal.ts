import { createClient } from '@supabase/supabase-js';
import * as dotenv from 'dotenv';
import { Client } from 'pg';

dotenv.config();

const supabase = createClient(process.env.SUPABASE_URL!, process.env.SUPABASE_SERVICE_ROLE!);

async function insertInBatches(tableName: string, data: any[]): Promise<number> {
  const batchSize = 50;
  let totalInserted = 0;

  for (let i = 0; i < data.length; i += batchSize) {
    const batch = data.slice(i, i + batchSize);

    try {
      const { error } = await supabase
        .from(tableName)
        .insert(batch);

      if (error) {
        console.error(`   âŒ Error inserting batch for ${tableName}:`, error.message);
        if (batch.length > 0) {
          console.error(`   First item structure:`, JSON.stringify(batch[0], null, 2));
        }
        continue;
      }

      totalInserted += batch.length;
      console.log(`   âœ… Inserted ${batch.length} records for ${tableName} (total: ${totalInserted})`);

    } catch (batchError: any) {
      console.error(`   âŒ Batch error for ${tableName}:`, batchError.message);
    }
  }

  return totalInserted;
}

async function migrateOperationsMinimal() {
  console.log('ðŸš€ Starting minimal operations migration (required fields only)...\n');

  const sourceClient = new Client({
    host: process.env.SOURCE_DB_HOST!,
    port: parseInt(process.env.SOURCE_DB_PORT!),
    user: process.env.SOURCE_DB_USER!,
    password: process.env.SOURCE_DB_PASSWORD!,
    database: process.env.SOURCE_DB_NAME!,
  });

  try {
    await sourceClient.connect();
    console.log('âœ… Connected to source database');

    // Get a default case to use for operations
    console.log('\nðŸ“‹ Getting default case for operations...');
    const { data: defaultCase } = await supabase
      .from('cases')
      .select('id')
      .limit(1)
      .single();

    if (!defaultCase) {
      console.log('âŒ No cases found - cannot migrate operations without case_id');
      return;
    }

    console.log(`âœ… Using default case: ${defaultCase.id}`);

    // Get source operations data
    console.log('\nâš™ï¸ Fetching operations from dispatch_operation...');
    const operationsResult = await sourceClient.query(`
      SELECT id, type, made_at, price, sq_order_id, sq_payment_id, card_brand, card_last, payment_id
      FROM dispatch_operation
      ORDER BY id;
    `);

    console.log(`ðŸ“Š Found ${operationsResult.rows.length} operations to migrate`);

    // Get type distribution
    const typeStats = operationsResult.rows.reduce((acc: any, row: any) => {
      acc[row.type] = (acc[row.type] || 0) + 1;
      return acc;
    }, {});
    console.log(`ðŸ“ˆ Operation types in source:`, typeStats);

    const totalValue = operationsResult.rows.reduce((sum: number, row: any) => sum + parseFloat(row.price || '0'), 0);
    console.log(`ðŸ’° Total value: $${totalValue.toFixed(2)}`);

    // Create MINIMAL operation records with only known working fields
    const operations = operationsResult.rows.map((row: any) => {
      let operationType = 'payment'; // Default
      if (row.type === 1) operationType = 'payment';
      else if (row.type === 2) operationType = 'refund';

      // MINIMAL RECORD - only the fields we know work
      return {
        case_id: defaultCase.id, // Required
        operation_type: operationType // Required
        // No other fields to avoid schema cache issues
      };
    });

    console.log(`\nðŸ“¦ Prepared ${operations.length} minimal operations for insertion`);

    // Show sample operation
    if (operations.length > 0) {
      console.log('\nðŸ“‹ Sample minimal operation:');
      console.log(JSON.stringify(operations[0], null, 2));
    }

    // Insert operations in batches
    console.log('\nâš¡ Starting batch insertion (minimal fields only)...');
    const totalInserted = await insertInBatches('operations', operations);

    // Final summary
    console.log('\nðŸ“Š OPERATIONS MIGRATION SUMMARY:');
    console.log(`âœ… Source operations: ${operationsResult.rows.length}`);
    console.log(`âœ… Successfully migrated: ${totalInserted}`);
    console.log(`âœ… Success rate: ${((totalInserted / operationsResult.rows.length) * 100).toFixed(1)}%`);
    console.log(`ðŸ’° Source value: $${totalValue.toFixed(2)} (preserved in migration_mappings)`);

    // Show operation type distribution for migrated data
    if (totalInserted > 0) {
      const migratedOps = operations.slice(0, totalInserted);
      const typeDistribution = migratedOps.reduce((acc: any, op: any) => {
        acc[op.operation_type] = (acc[op.operation_type] || 0) + 1;
        return acc;
      }, {});

      console.log('\nðŸ“ˆ Migrated operation types:');
      Object.entries(typeDistribution).forEach(([type, count]) => {
        console.log(`   ${type}: ${count} operations`);
      });
    }

    // Create legacy mapping for reference (in migration_mappings if needed)
    if (totalInserted > 0) {
      console.log('\nðŸ“‹ Creating legacy mapping for reference...');

      // Get the inserted operations to match them with source data
      const { data: insertedOps } = await supabase
        .from('operations')
        .select('id, case_id, operation_type')
        .limit(totalInserted);

      if (insertedOps && insertedOps.length > 0) {
        console.log(`âœ… Successfully linked ${insertedOps.length} operations`);

        // Store mapping information for future reference
        const mappingData = operationsResult.rows.slice(0, totalInserted).map((sourceRow: any, index: number) => ({
          legacy_table: 'dispatch_operation',
          legacy_id: sourceRow.id,
          target_table: 'operations',
          target_id: insertedOps[index]?.id,
          migration_metadata: {
            sq_order_id: sourceRow.sq_order_id,
            sq_payment_id: sourceRow.sq_payment_id,
            card_brand: sourceRow.card_brand,
            card_last: sourceRow.card_last,
            price: parseFloat(sourceRow.price || '0'),
            payment_id: sourceRow.payment_id,
            made_at: sourceRow.made_at
          }
        }));

        // Try to insert mapping data
        try {
          const { error: mappingError } = await supabase
            .from('migration_mappings')
            .insert(mappingData);

          if (!mappingError) {
            console.log(`âœ… Created ${mappingData.length} legacy mappings`);
          }
        } catch (e) {
          console.log(`âš ï¸  Could not store legacy mappings (table may not exist)`);
        }
      }
    }

    // Verify final count
    const { count: finalCount } = await supabase
      .from('operations')
      .select('*', { count: 'exact', head: true });

    console.log(`ðŸ“¦ Final operations count in database: ${finalCount || 0}`);

    if (totalInserted > 0) {
      console.log('\nðŸŽ‰ Operations migration completed successfully!');
      console.log('ðŸ’¡ Note: Detailed payment info preserved in legacy mappings');
      console.log(`ðŸ’° Financial audit trail maintained for $${totalValue.toFixed(2)}`);
    } else {
      console.log('\nâš ï¸  Operations migration completed with issues - check errors above');
    }

    return totalInserted;

  } catch (error: any) {
    console.error('âŒ Operations migration failed:', error);
    throw error;
  } finally {
    await sourceClient.end();
    console.log('ðŸ”Œ Disconnected from source database');
  }
}

// Run the migration
if (require.main === module) {
  migrateOperationsMinimal().catch(error => {
    console.error('Fatal error:', error);
    process.exit(1);
  });
}

export default migrateOperationsMinimal;